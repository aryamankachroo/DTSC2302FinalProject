{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e6cf6e",
   "metadata": {},
   "source": [
    "Hello, this is our Data Science Studio Final Project. Today, we will be performing an analysis and creating mutliple classification models using the DCWP Consumer Complaints Dataset. Consumer complaints are an important signal for identifying patterns of dissatisfaction, fraud, or misconduct among businesses. By analyzing consumer complaints, agencies and companies can improve cutomer service, ensure compliance with regulations, and protect consumers from unfair practices.\n",
    "\n",
    "### The aim of this analysis\n",
    "\n",
    "Today, we will assume the role of data analysts working for a consumer protection agency. We need to analyze the DCWP Consumer Complaints dataset and develop classification models that could accurately predict the status of a consumer complaint based on various factors such as the type of business, type of complaint, and subission methods. We will also evaluate the different models to see which one performs better. Some of the questions we aim to answer are:\n",
    "- Which features are most import in determining the final status of a consumer complaint?\n",
    "- Can we identify any patterns or trends in the types of complaints received across different business categories?\n",
    "- Can we build a predictive model that can accurately classify the complaint status based on the available information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1309c",
   "metadata": {},
   "source": [
    "### Introducing the Dataset\n",
    "\n",
    "The dataset we will be using for this project is the DCWP Consumer Complaints Datset, which we retrieved from the NYC Open Data Portal using an API. The Department of Consumer and WOrker Protection (DCWP) records complaints filed by consumers against businesses operating in New York City. This dataset provides valuable insights into consumer issues and business compliance across various inductries.\n",
    "\n",
    "The features we will be using in this project are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc6d1f",
   "metadata": {},
   "source": [
    "Let's start by exploring the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "from sodapy import Socrata\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e2291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "# Creating a client object to make API requests\n",
    "\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98767e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ID of the dataset\n",
    "\n",
    "dataset_id = \"nre2-6m2s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204152d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data\n",
    "\n",
    "results = client.get(dataset_id, limit = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f346c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it into a dataframe\n",
    "\n",
    "df = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2429a30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>intake_date</th>\n",
       "      <th>intake_channel</th>\n",
       "      <th>_311_sr_number</th>\n",
       "      <th>business_category</th>\n",
       "      <th>complaint_code</th>\n",
       "      <th>business_unique_id</th>\n",
       "      <th>business_name</th>\n",
       "      <th>result_date</th>\n",
       "      <th>result</th>\n",
       "      <th>...</th>\n",
       "      <th>census_block_2010_</th>\n",
       "      <th>census_tract_2010_</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>street2</th>\n",
       "      <th>apt_suite</th>\n",
       "      <th>unit_type</th>\n",
       "      <th>refund_amount</th>\n",
       "      <th>street3</th>\n",
       "      <th>contract_cancelled_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>057329-2025-CMPL</td>\n",
       "      <td>2025-02-24T00:00:00.000</td>\n",
       "      <td>311</td>\n",
       "      <td>311-22038149</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>Price Gouging</td>\n",
       "      <td>BA-1722078-2025</td>\n",
       "      <td>pateizia restaurant</td>\n",
       "      <td>2025-02-24T00:00:00.000</td>\n",
       "      <td>Referred</td>\n",
       "      <td>...</td>\n",
       "      <td>8002</td>\n",
       "      <td>66</td>\n",
       "      <td>40.73991700211976</td>\n",
       "      <td>-73.97939843013278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>057324-2025-CMPL</td>\n",
       "      <td>2025-02-24T00:00:00.000</td>\n",
       "      <td>311</td>\n",
       "      <td>311-22036709</td>\n",
       "      <td>Supermarket</td>\n",
       "      <td>Overcharge</td>\n",
       "      <td>BA-1722116-2025</td>\n",
       "      <td>A &amp; Y Embassy Food Corp.</td>\n",
       "      <td>2025-02-24T00:00:00.000</td>\n",
       "      <td>Referred</td>\n",
       "      <td>...</td>\n",
       "      <td>3005</td>\n",
       "      <td>539</td>\n",
       "      <td>40.707436555829005</td>\n",
       "      <td>-73.9154949964182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>057319-2025-CMPL</td>\n",
       "      <td>2025-02-24T00:00:00.000</td>\n",
       "      <td>311</td>\n",
       "      <td>311-22036401</td>\n",
       "      <td>Misc Non-Food Retail</td>\n",
       "      <td>Non-Delivery of Goods - N01</td>\n",
       "      <td>BA-1722067-2025</td>\n",
       "      <td>BURGER KING</td>\n",
       "      <td>2025-02-24T00:00:00.000</td>\n",
       "      <td>Complaint Review Complete</td>\n",
       "      <td>...</td>\n",
       "      <td>1006</td>\n",
       "      <td>109</td>\n",
       "      <td>40.74997771919704</td>\n",
       "      <td>-73.98792375849172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>057305-2025-CMPL</td>\n",
       "      <td>2025-02-23T00:00:00.000</td>\n",
       "      <td>311</td>\n",
       "      <td>311-22034563</td>\n",
       "      <td>Supermarket</td>\n",
       "      <td>Non-Delivery of Goods - N01</td>\n",
       "      <td>BA-1722054-2025</td>\n",
       "      <td>OCEAN BAY MARKET INC.</td>\n",
       "      <td>2025-02-24T00:00:00.000</td>\n",
       "      <td>Referred</td>\n",
       "      <td>...</td>\n",
       "      <td>1000</td>\n",
       "      <td>392</td>\n",
       "      <td>40.59765876127</td>\n",
       "      <td>-73.9611496265785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>057273-2025-CMPL</td>\n",
       "      <td>2025-02-23T00:00:00.000</td>\n",
       "      <td>311</td>\n",
       "      <td>311-22025951</td>\n",
       "      <td>Dry Cleaners</td>\n",
       "      <td>Lost/Stolen/Damaged Property</td>\n",
       "      <td>BA-1722025-2025</td>\n",
       "      <td>Vital tailor shop</td>\n",
       "      <td>2025-02-24T00:00:00.000</td>\n",
       "      <td>Insufficient Info Received</td>\n",
       "      <td>...</td>\n",
       "      <td>2001</td>\n",
       "      <td>101</td>\n",
       "      <td>40.65667828908096</td>\n",
       "      <td>-74.00194621940757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          record_id              intake_date intake_channel _311_sr_number  \\\n",
       "0  057329-2025-CMPL  2025-02-24T00:00:00.000            311   311-22038149   \n",
       "1  057324-2025-CMPL  2025-02-24T00:00:00.000            311   311-22036709   \n",
       "2  057319-2025-CMPL  2025-02-24T00:00:00.000            311   311-22036401   \n",
       "3  057305-2025-CMPL  2025-02-23T00:00:00.000            311   311-22034563   \n",
       "4  057273-2025-CMPL  2025-02-23T00:00:00.000            311   311-22025951   \n",
       "\n",
       "      business_category                complaint_code business_unique_id  \\\n",
       "0            Restaurant                 Price Gouging    BA-1722078-2025   \n",
       "1           Supermarket                    Overcharge    BA-1722116-2025   \n",
       "2  Misc Non-Food Retail   Non-Delivery of Goods - N01    BA-1722067-2025   \n",
       "3           Supermarket   Non-Delivery of Goods - N01    BA-1722054-2025   \n",
       "4          Dry Cleaners  Lost/Stolen/Damaged Property    BA-1722025-2025   \n",
       "\n",
       "              business_name              result_date  \\\n",
       "0       pateizia restaurant  2025-02-24T00:00:00.000   \n",
       "1  A & Y Embassy Food Corp.  2025-02-24T00:00:00.000   \n",
       "2               BURGER KING  2025-02-24T00:00:00.000   \n",
       "3     OCEAN BAY MARKET INC.  2025-02-24T00:00:00.000   \n",
       "4         Vital tailor shop  2025-02-24T00:00:00.000   \n",
       "\n",
       "                       result  ... census_block_2010_ census_tract_2010_  \\\n",
       "0                    Referred  ...               8002                 66   \n",
       "1                    Referred  ...               3005                539   \n",
       "2   Complaint Review Complete  ...               1006                109   \n",
       "3                    Referred  ...               1000                392   \n",
       "4  Insufficient Info Received  ...               2001                101   \n",
       "\n",
       "             latitude           longitude street2 apt_suite unit_type  \\\n",
       "0   40.73991700211976  -73.97939843013278     NaN       NaN       NaN   \n",
       "1  40.707436555829005   -73.9154949964182     NaN       NaN       NaN   \n",
       "2   40.74997771919704  -73.98792375849172     NaN       NaN       NaN   \n",
       "3      40.59765876127   -73.9611496265785     NaN       NaN       NaN   \n",
       "4   40.65667828908096  -74.00194621940757     NaN       NaN       NaN   \n",
       "\n",
       "  refund_amount street3 contract_cancelled_amount  \n",
       "0           NaN     NaN                       NaN  \n",
       "1           NaN     NaN                       NaN  \n",
       "2           NaN     NaN                       NaN  \n",
       "3           NaN     NaN                       NaN  \n",
       "4           NaN     NaN                       NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffa0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 33)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the initial shape of the dataset\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09760d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['record_id',\n",
       " 'intake_date',\n",
       " 'intake_channel',\n",
       " '_311_sr_number',\n",
       " 'business_category',\n",
       " 'complaint_code',\n",
       " 'business_unique_id',\n",
       " 'business_name',\n",
       " 'result_date',\n",
       " 'result',\n",
       " 'referred_to',\n",
       " 'address_type',\n",
       " 'building_nbr',\n",
       " 'street1',\n",
       " 'city',\n",
       " 'state',\n",
       " 'postcode',\n",
       " 'borough',\n",
       " 'community_board',\n",
       " 'council_district',\n",
       " 'bin',\n",
       " 'bbl',\n",
       " 'nta',\n",
       " 'census_block_2010_',\n",
       " 'census_tract_2010_',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'street2',\n",
       " 'apt_suite',\n",
       " 'unit_type',\n",
       " 'refund_amount',\n",
       " 'street3',\n",
       " 'contract_cancelled_amount']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The columns\n",
    "\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac6deb",
   "metadata": {},
   "source": [
    "### Pre-processing the Data\n",
    "\n",
    "Before building a classification model, it is important to preprocess the data in a careful matter to ensure it is clean, consistent, and suitable for the algorithms we will be using in this porject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445f0f7",
   "metadata": {},
   "source": [
    "We will preprocess the data by handling missing values, dropping irrelevant features, simplifying the target variable, encode categorical variables, and doing feature engineering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7f8deaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contract_cancelled_amount    4998\n",
       "street3                      4996\n",
       "refund_amount                4931\n",
       "street2                      4846\n",
       "unit_type                    4795\n",
       "apt_suite                    4595\n",
       "referred_to                  2814\n",
       "_311_sr_number               1427\n",
       "bbl                           815\n",
       "bin                           815\n",
       "nta                           697\n",
       "census_block_2010_            697\n",
       "complaint_code                673\n",
       "building_nbr                  643\n",
       "census_tract_2010_            555\n",
       "community_board               555\n",
       "council_district              555\n",
       "longitude                     509\n",
       "latitude                      509\n",
       "borough                       489\n",
       "city                          191\n",
       "business_unique_id            145\n",
       "business_name                 144\n",
       "state                          51\n",
       "postcode                       50\n",
       "street1                        50\n",
       "address_type                   45\n",
       "business_category               8\n",
       "record_id                       0\n",
       "result                          0\n",
       "intake_channel                  0\n",
       "intake_date                     0\n",
       "result_date                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking and sorting missing values\n",
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3a7110e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping irrelevant columns\n",
    "# Feature Selection\n",
    "columns_to_drop = [\n",
    "    'record_id', '_311_sr_number', 'business_unique_id', 'building_nbr',\n",
    "    'street1', 'street2', 'street3', 'apt_suite', 'bin', 'bbl', 'latitude', 'longitude',\n",
    "    'borough', 'community_board', 'council_district', 'nta',\n",
    "    'census_block_2010_', 'census_tract_2010_', 'address_type', 'state', 'postcode', 'referred_to', 'city'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Checking new shape\n",
    "# we can see that we have 10 columns after we drop the list\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5d9efeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contract_cancelled_amount    4998\n",
       "refund_amount                4931\n",
       "unit_type                    4795\n",
       "complaint_code                673\n",
       "business_name                 144\n",
       "business_category               8\n",
       "intake_channel                  0\n",
       "intake_date                     0\n",
       "result_date                     0\n",
       "result                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for nulls after removing irrelevant features\n",
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d0d27fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with 50% missing values by setting a threshold\n",
    "threshold = len(df) * 0.5\n",
    "df = df.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# filling missing values for categorical columns with 'Unknown'\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ab171628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intake_date          0\n",
       "intake_channel       0\n",
       "business_category    0\n",
       "complaint_code       0\n",
       "business_name        0\n",
       "result_date          0\n",
       "result               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final check for nulls\n",
    "# we can see that we do not have missing values anymore \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ad73f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "# Creating the column resolution_days to see how many days the issue took to be resolved\n",
    "\n",
    "df['intake_date'] = pd.to_datetime(df['intake_date'], errors = 'coerce')\n",
    "df['result_date'] = pd.to_datetime(df['result_date'], errors = 'coerce')\n",
    "\n",
    "df['resolution_days'] = (df['result_date']-df['intake_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9d8eb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifying the target class variable to improve model performance\n",
    "# this helps balance the dataset and avoid overfitting to very small classes\n",
    "\n",
    "positive_responses = ['Resolved', 'Reduced', 'Goods', 'Store Credit', 'Cash Amount', 'Took Action', 'Consumer Restitution']\n",
    "\n",
    "df['vendor_responded'] = df['result'].apply(lambda x: 1 if any(keyword.lower() in x.lower() for keyword in positive_responses) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "feacfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing both Label Encoders\n",
    "\n",
    "le_business_category = LabelEncoder()\n",
    "le_complaint_code = LabelEncoder()\n",
    "\n",
    "# Applying Label Encoding to categorical columns needed\n",
    "\n",
    "df['business_category_encoded'] = le_business_category.fit_transform(df['business_category'].astype(str))\n",
    "df['complaint_code_encoded'] = le_business_category.fit_transform(df['complaint_code'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "af0b0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of the features we want to use\n",
    "features = ['business_category_encoded', 'complaint_code_encoded', 'resolution_days']\n",
    "\n",
    "# Defining the features (X) and the target variable (y)\n",
    "\n",
    "X = df[features]\n",
    "y = df['vendor_responded'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "50f5c6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (4000, 3)\n",
      "Testing set size: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify=y) \n",
    "\n",
    "# checking the sizes of both the training and testing data\n",
    "# same amount of columns\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a446bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Results:\n",
      "----------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       976\n",
      "           1       0.31      0.17      0.22        24\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.64      0.58      0.60      1000\n",
      "weighted avg       0.96      0.97      0.97      1000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[967   9]\n",
      " [ 20   4]]\n",
      "\n",
      "Accuracy Score: 0.971\n"
     ]
    }
   ],
   "source": [
    "# Creating a Random Forest Classifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state = 42) # Initializing\n",
    "rf_model.fit(X_train, y_train) # Training\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test) # Making predictions\n",
    "\n",
    "# Evaluating the model\n",
    "\n",
    "print(\"Random Forest Classifier Results:\")\n",
    "print(\"----------------------------------\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5c4e8b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results:\n",
      "-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       976\n",
      "           1       1.00      0.04      0.08        24\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.99      0.52      0.53      1000\n",
      "weighted avg       0.98      0.98      0.97      1000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[976   0]\n",
      " [ 23   1]]\n",
      "\n",
      "Accuracy Score: 0.977\n"
     ]
    }
   ],
   "source": [
    "# Creating a Logistic Regression model\n",
    "\n",
    "logreg_model = LogisticRegression(max_iter=1000, random_state=42) # Initializing\n",
    "logreg_model.fit(X_train, y_train) # Training\n",
    "\n",
    "y_pred_logreg = logreg_model.predict(X_test) # Making predictions\n",
    "\n",
    "# Evaluating the model\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(\"-----------------------------\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_logreg))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_logreg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "df72a311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Classifier Results:\n",
      "----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       976\n",
      "           1       0.36      0.33      0.35        24\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.67      0.66      0.67      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[962  14]\n",
      " [ 16   8]]\n",
      "\n",
      "Accuracy Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Creating a XGBoost model\n",
    "\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42) # Initializing\n",
    "xgb_model.fit(X_train, y_train) # Training\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test) # Making predictions\n",
    "\n",
    "# Evaluating the model\n",
    "\n",
    "print(\"\\nXGBoost Classifier Results:\")\n",
    "print(\"----------------------------\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_xgb))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Theminevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
